{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQW9FXVjcdMTZhCIKllhqu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahd1995913/Identifying-Machine-Generated-Text-/blob/main/Part1_multilingual_framework_for_detecting_biases_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "WxGJ_JC8pR0H",
        "outputId": "2847d45b-3b80-4811-e952-9283ad82fee5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-342384494.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Core libs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mensure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mensure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transformers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mensure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-342384494.py\u001b[0m in \u001b[0;36mensure\u001b[0;34m(pkg, import_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimport_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--quiet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============================================ Bias Analyzer\n",
        "\n",
        "# ---- Auto install dependencies (safe & idempotent) ----\n",
        "import sys, subprocess, importlib\n",
        "\n",
        "def ensure(pkg, import_name=None):\n",
        "    import_name = import_name or pkg\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", pkg])\n",
        "        importlib.invalidate_caches()\n",
        "\n",
        "# Core libs\n",
        "ensure(\"torch\")\n",
        "ensure(\"transformers\")\n",
        "ensure(\"numpy\")\n",
        "ensure(\"scipy\")\n",
        "ensure(\"nltk\")\n",
        "# Detoxify for toxicity (has CPU-only fallback if no GPU)\n",
        "ensure(\"detoxify\")\n",
        "\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from statistics import mean, pstdev\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from detoxify import Detoxify\n",
        "import nltk\n",
        "\n",
        "# NLTK lightweight setup (we avoid punkt to keep footprint small)\n",
        "# For sentiment we need vader_lexicon only\n",
        "try:\n",
        "    nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
        "except LookupError:\n",
        "    nltk.download(\"vader_lexicon\", quiet=True)\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# ---------------- Utilities ----------------\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    # Lightweight sentence splitter to avoid large NLTK punkt\n",
        "    # Splits on .?! while preserving reasonable tokens\n",
        "    raw = re.split(r'(?<=[\\.\\?\\!])\\s+', text.strip())\n",
        "    # Clean and filter\n",
        "    return [s.strip() for s in raw if s.strip()]\n",
        "\n",
        "def tokenize_words(text: str) -> List[str]:\n",
        "    return re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ0-9']+\", text.lower())\n",
        "\n",
        "def type_token_ratio(tokens: List[str]) -> float:\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "    return len(set(tokens)) / max(1, len(tokens))\n",
        "\n",
        "def repetition_ratio(tokens: List[str], n: int = 3) -> float:\n",
        "    \"\"\"\n",
        "    Measures n-gram repetition (0..1). Higher -> more repetition (AI often repeats).\n",
        "    \"\"\"\n",
        "    if len(tokens) < n:\n",
        "        return 0.0\n",
        "    ngrams = [\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "    if not ngrams:\n",
        "        return 0.0\n",
        "    unique = len(set(ngrams))\n",
        "    return 1.0 - (unique / len(ngrams))\n",
        "\n",
        "def sentence_length_stats(sentences: List[str]) -> Dict[str, float]:\n",
        "    lengths = [len(tokenize_words(s)) for s in sentences if s.strip()]\n",
        "    if not lengths:\n",
        "        return {\"avg\": 0.0, \"std\": 0.0, \"cv\": 0.0}\n",
        "    avg = mean(lengths)\n",
        "    std = pstdev(lengths) if len(lengths) > 1 else 0.0\n",
        "    cv = (std / avg) if avg > 0 else 0.0\n",
        "    return {\"avg\": float(avg), \"std\": float(std), \"cv\": float(cv)}\n",
        "\n",
        "def softclip(x, lo, hi):\n",
        "    return max(lo, min(hi, x))\n",
        "\n",
        "# ---------------- Perplexity via GPT-2 ----------------\n",
        "# We use GPT-2 to compute perplexity (lower perplexity -> more \"model-like\")\n",
        "# Robustly handles long texts by sliding window.\n",
        "class PerplexityScorer:\n",
        "    def __init__(self, model_name: str = \"gpt2\", max_length: int = 1024, stride: int = 512, device: str = None):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.max_length = max_length\n",
        "        self.stride = stride\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def perplexity(self, text: str) -> float:\n",
        "        if not text.strip():\n",
        "            return float(\"inf\")\n",
        "        encodings = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        input_ids = encodings.input_ids.to(self.device)\n",
        "        nlls = []\n",
        "        for i in range(0, input_ids.size(1), self.stride):\n",
        "            begin_loc = max(i + self.stride - self.max_length, 0)\n",
        "            end_loc = min(i + self.stride, input_ids.size(1))\n",
        "            trg_len = end_loc - i\n",
        "            if trg_len <= 0:\n",
        "                continue\n",
        "            input_ids_slice = input_ids[:, begin_loc:end_loc]\n",
        "            target_ids = input_ids_slice.clone()\n",
        "            target_ids[:, :-trg_len] = -100  # mask everything except target region\n",
        "            outputs = self.model(input_ids_slice, labels=target_ids)\n",
        "            nll = outputs.loss * trg_len\n",
        "            nlls.append(nll)\n",
        "        if not nlls:\n",
        "            return float(\"inf\")\n",
        "        ppl = torch.exp(torch.stack(nlls).sum() / max(1, input_ids.size(1))).item()\n",
        "        # Guard against numeric issues\n",
        "        if math.isinf(ppl) or math.isnan(ppl):\n",
        "            return 1e6\n",
        "        return float(ppl)\n",
        "\n",
        "# ---------------- Toxicity & Sentiment ----------------\n",
        "\n",
        "class RobustDetoxify:\n",
        "    \"\"\"\n",
        "    Wrap Detoxify with CPU fallback and graceful degradation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Use 'original' weights (multi-label) suitable for general toxicity\n",
        "        try:\n",
        "            self.model = Detoxify('original')\n",
        "            self.ok = True\n",
        "        except Exception:\n",
        "            self.model = None\n",
        "            self.ok = False\n",
        "\n",
        "    def score(self, text: str) -> Dict[str, float]:\n",
        "        if not self.ok or not text.strip():\n",
        "            return {\"toxicity\": 0.0, \"severe_toxicity\": 0.0, \"obscene\": 0.0,\n",
        "                    \"threat\": 0.0, \"insult\": 0.0, \"identity_attack\": 0.0}\n",
        "        try:\n",
        "            out = self.model.predict(text)\n",
        "            # Ensure presence of keys\n",
        "            for k in [\"toxicity\",\"severe_toxicity\",\"obscene\",\"threat\",\"insult\",\"identity_attack\"]:\n",
        "                if k not in out:\n",
        "                    out[k] = 0.0\n",
        "            return {k: float(out[k]) for k in out}\n",
        "        except Exception:\n",
        "            return {\"toxicity\": 0.0, \"severe_toxicity\": 0.0, \"obscene\": 0.0,\n",
        "                    \"threat\": 0.0, \"insult\": 0.0, \"identity_attack\": 0.0}\n",
        "\n",
        "# ---------------- Bias Lexicon ----------------\n",
        "\n",
        "PROTECTED_TERMS = set([\n",
        "    # Gender & identity\n",
        "    \"woman\",\"women\",\"man\",\"men\",\"female\",\"male\",\"nonbinary\",\"trans\",\"transgender\",\"lgbt\",\"gay\",\"lesbian\",\"bisexual\",\"queer\",\n",
        "    # Race/ethnicity (examples; non-exhaustive, lowercase)\n",
        "    \"black\",\"white\",\"asian\",\"latino\",\"hispanic\",\"arab\",\"indian\",\"african\",\"european\",\"indigenous\",\"native\",\"jewish\",\"jews\",\"muslim\",\"christian\",\"christians\",\n",
        "    # Religion (more)\n",
        "    \"islam\",\"christianity\",\"judaism\",\"buddhist\",\"hindu\",\"sikh\",\n",
        "    # Age/disability\n",
        "    \"disabled\",\"disability\",\"handicapped\",\"elderly\",\"old\",\"young\",\"youth\",\"senior\",\n",
        "    # Nationality (examples)\n",
        "    \"american\",\"british\",\"irish\",\"french\",\"german\",\"italian\",\"spanish\",\"russian\",\"ukrainian\",\"chinese\",\"japanese\",\"korean\",\"turkish\",\"saudi\",\"emirati\",\"egyptian\",\"jordanian\",\"moroccan\"\n",
        "])\n",
        "\n",
        "NEGATION_TERMS = set([\"not\",\"never\",\"no\",\"none\",\"hardly\",\"barely\",\"without\",\"less\"])\n",
        "\n",
        "def contains_protected_terms(text: str) -> bool:\n",
        "    tokens = tokenize_words(text)\n",
        "    return any(t in PROTECTED_TERMS for t in tokens)\n",
        "\n",
        "# ---------------- Feature Engineering ----------------\n",
        "\n",
        "@dataclass\n",
        "class TextFeatures:\n",
        "    perplexity: float\n",
        "    ttr: float\n",
        "    repetition: float\n",
        "    sent_cv: float\n",
        "    avg_sent_len: float\n",
        "\n",
        "def extract_features(text: str, ppl_scorer: PerplexityScorer) -> TextFeatures:\n",
        "    sentences = split_sentences(text)\n",
        "    tokens = tokenize_words(text)\n",
        "    stats = sentence_length_stats(sentences)\n",
        "    ppl = ppl_scorer.perplexity(text)\n",
        "    return TextFeatures(\n",
        "        perplexity=ppl,\n",
        "        ttr=type_token_ratio(tokens),\n",
        "        repetition=repetition_ratio(tokens, n=3),\n",
        "        sent_cv=stats[\"cv\"],\n",
        "        avg_sent_len=stats[\"avg\"]\n",
        "    )\n",
        "\n",
        "# ---------------- AI Probability (Heuristic Model) ----------------\n",
        "# We map features to a probability via a simple, well-behaved logistic model.\n",
        "# Intuition:\n",
        "#   • Lower perplexity  -> more AI-like\n",
        "#   • Lower burstiness (CV) -> more AI-like\n",
        "#   • Higher repetition -> more AI-like\n",
        "#   • Lower TTR         -> more AI-like\n",
        "\n",
        "def sigmoid(x): return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def ai_probability(feat: TextFeatures) -> float:\n",
        "    # Normalize features to stable ranges\n",
        "    # Perplexity is unbounded & skewed; clip to [5, 200] then invert.\n",
        "    ppl = softclip(feat.perplexity, 5, 200)\n",
        "    ppl_norm = (ppl - 5) / (200 - 5)  # 0 (very low ppl) -> 1 (high ppl)\n",
        "    # We want lower perplexity => higher AI-ness, so invert:\n",
        "    ppl_ai = 1.0 - ppl_norm\n",
        "\n",
        "    # CV (0..2 typical); cap at 1.5\n",
        "    cv = softclip(feat.sent_cv, 0.0, 1.5) / 1.5\n",
        "    cv_ai = 1.0 - cv  # lower CV -> more AI-like\n",
        "\n",
        "    # Repetition already 0..1 (higher -> AI-like)\n",
        "    rep_ai = softclip(feat.repetition, 0.0, 1.0)\n",
        "\n",
        "    # TTR (0..1) (lower -> AI-like)\n",
        "    ttr_ai = 1.0 - softclip(feat.ttr, 0.0, 1.0)\n",
        "\n",
        "    # Weighted sum -> logistic\n",
        "    # Weights tuned for balanced influence\n",
        "    z = (2.2 * ppl_ai) + (1.6 * cv_ai) + (1.2 * rep_ai) + (1.0 * ttr_ai) - 2.0\n",
        "    return float(softclip(sigmoid(z), 0.01, 0.99))\n",
        "\n",
        "# ---------------- Mixed Authorship Detection ----------------\n",
        "\n",
        "def segment_ai_mix(text: str, ppl_scorer: PerplexityScorer) -> Dict[str, Any]:\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return {\"ai_like\": 0.0, \"human_like\": 0.0, \"label\": \"insufficient_text\"}\n",
        "    # Group into chunks of ~4 sentences for stable scoring\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), 4):\n",
        "        chunk = \" \".join(sentences[i:i+4])\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "    probs = []\n",
        "    for ch in chunks:\n",
        "        f = extract_features(ch, ppl_scorer)\n",
        "        probs.append(ai_probability(f))\n",
        "    if not probs:\n",
        "        return {\"ai_like\": 0.0, \"human_like\": 0.0, \"label\": \"insufficient_text\"}\n",
        "    ai_frac = float(np.mean([p > 0.6 for p in probs]))\n",
        "    human_frac = float(np.mean([p < 0.4 for p in probs]))\n",
        "    if ai_frac >= 0.3 and human_frac >= 0.3:\n",
        "        label = \"mixed\"\n",
        "    elif ai_frac >= 0.5:\n",
        "        label = \"ai_generated_likely\"\n",
        "    elif human_frac >= 0.5:\n",
        "        label = \"human_written_likely\"\n",
        "    else:\n",
        "        label = \"unclear\"\n",
        "    return {\"ai_like\": ai_frac, \"human_like\": human_frac, \"label\": label, \"chunk_probs\": probs}\n",
        "\n",
        "# ---------------- Bias Scoring ----------------\n",
        "\n",
        "def bias_scores(text: str, tox_model: RobustDetoxify) -> Dict[str, Any]:\n",
        "    # Toxicity (0..1 for each)\n",
        "    tox = tox_model.score(text)\n",
        "    toxicity = float(tox.get(\"toxicity\", 0.0))\n",
        "    identity_attack = float(tox.get(\"identity_attack\", 0.0))\n",
        "    insult = float(tox.get(\"insult\", 0.0))\n",
        "    severe = float(tox.get(\"severe_toxicity\", 0.0))\n",
        "    threat = float(tox.get(\"threat\", 0.0))\n",
        "\n",
        "    # Sentiment extremity (abs compound)\n",
        "    sent = sia.polarity_scores(text)\n",
        "    sentiment_extremity = abs(float(sent.get(\"compound\", 0.0)))  # 0..1\n",
        "\n",
        "    # Mentions of protected groups\n",
        "    has_protected = contains_protected_terms(text)\n",
        "\n",
        "    # Bias aggregator:\n",
        "    # Base toxicity weight\n",
        "    base = 0.5 * toxicity + 0.1 * severe + 0.1 * threat + 0.15 * insult + 0.15 * identity_attack\n",
        "    # If protected groups mentioned, amplify by sentiment extremity (negative contexts tend to be higher risk)\n",
        "    protected_amp = (0.4 * sentiment_extremity) if has_protected else (0.15 * sentiment_extremity)\n",
        "    score = softclip(base + protected_amp, 0.0, 1.0)\n",
        "\n",
        "    # Map to 0..100\n",
        "    overall = round(100.0 * score, 2)\n",
        "\n",
        "    return {\n",
        "        \"overall_bias_score_0_100\": overall,\n",
        "        \"toxicity\": round(100 * toxicity, 2),\n",
        "        \"identity_attack\": round(100 * identity_attack, 2),\n",
        "        \"insult\": round(100 * insult, 2),\n",
        "        \"severe_toxicity\": round(100 * severe, 2),\n",
        "        \"threat\": round(100 * threat, 2),\n",
        "        \"sentiment_extremity_0_100\": round(100 * sentiment_extremity, 2),\n",
        "        \"mentions_protected_groups\": bool(has_protected)\n",
        "    }\n",
        "\n",
        "# ---------------- Main API ----------------\n",
        "\n",
        "def analyze_text(text: str) -> Dict[str, Any]:\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return {\"error\": \"Empty text. Please provide non-empty input.\"}\n",
        "\n",
        "    # Init scorers\n",
        "    ppl_scorer = PerplexityScorer(model_name=\"gpt2\", max_length=1024, stride=512)\n",
        "    tox_model = RobustDetoxify()\n",
        "\n",
        "    # Global features & AI probability\n",
        "    features = extract_features(text, ppl_scorer)\n",
        "    ai_prob = ai_probability(features)\n",
        "    mix = segment_ai_mix(text, ppl_scorer)\n",
        "\n",
        "    # Labeling\n",
        "    if mix[\"label\"] == \"mixed\":\n",
        "        label = \"mixed_authorship_suspected\"\n",
        "    else:\n",
        "        if ai_prob >= 0.65:\n",
        "            label = \"ai_generated_likely\"\n",
        "        elif ai_prob <= 0.35:\n",
        "            label = \"human_written_likely\"\n",
        "        else:\n",
        "            label = \"unclear\"\n",
        "\n",
        "    # Bias scoring\n",
        "    bias = bias_scores(text, tox_model)\n",
        "\n",
        "    # Package results\n",
        "    return {\n",
        "        \"ai_detection\": {\n",
        "            \"ai_probability_0_1\": round(ai_prob, 4),\n",
        "            \"label\": label,\n",
        "            \"global_features\": {\n",
        "                \"perplexity\": round(features.perplexity, 2),\n",
        "                \"type_token_ratio\": round(features.ttr, 4),\n",
        "                \"repetition_3gram\": round(features.repetition, 4),\n",
        "                \"sentence_length_cv\": round(features.sent_cv, 4),\n",
        "                \"avg_sentence_length_tokens\": round(features.avg_sent_len, 2),\n",
        "            },\n",
        "            \"segment_mix_analysis\": {\n",
        "                \"ai_like_fraction\": round(float(mix.get(\"ai_like\", 0.0)), 3),\n",
        "                \"human_like_fraction\": round(float(mix.get(\"human_like\", 0.0)), 3),\n",
        "                \"segment_label\": mix.get(\"label\", \"n/a\"),\n",
        "                \"segment_ai_probs\": [round(float(p), 3) for p in mix.get(\"chunk_probs\", [])]\n",
        "            }\n",
        "        },\n",
        "        \"bias_assessment\": bias\n",
        "    }\n",
        "\n",
        "# ---------------- Example Usage ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    # You can replace this sample with any text, or use input() for quick testing.\n",
        "    sample_text = (\n",
        "        \"Artificial intelligence models have transformed how we summarize and write content. \"\n",
        "        \"This paragraph is intentionally neutral. However, claims that any specific nationality is inherently better at science \"\n",
        "        \"are misguided and not supported by evidence.\"\n",
        "    )\n",
        "\n",
        "    print(\"=== AI & Bias Analyzer ===\")\n",
        "    print(\"Provide your text below. Press Enter to use the built-in sample.\\n\")\n",
        "    try:\n",
        "        user_inp = input(\"Paste text (or press Enter to analyze the sample): \").strip()\n",
        "    except EOFError:\n",
        "        user_inp = \"\"\n",
        "\n",
        "    text = user_inp if user_inp else sample_text\n",
        "    results = analyze_text(text)\n",
        "    import json\n",
        "    print(json.dumps(results, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "AI-generated Content & Bias Detection System\n",
        "--------------------------------------------\n",
        "This system detects:\n",
        "1. Whether text is written by a Human or AI (using Hugging Face zero-shot models).\n",
        "2. Estimates degree of bias in the text:\n",
        "   - Linguistic bias (negative/exclusionary words).\n",
        "   - Religious or cultural bias.\n",
        "   - Lack of diversity in sources/terms.\n",
        "\n",
        "Author: Shahd\n",
        "\"\"\"\n",
        "\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# ------------------------------\n",
        "# Load Hugging Face Pipelines\n",
        "# ------------------------------\n",
        "# For AI/Human classification (zero-shot approach)\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Sentiment analysis to help detect negative connotations\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# ------------------------------\n",
        "# Bias Dictionaries (demo version)\n",
        "# ------------------------------\n",
        "bias_keywords = {\n",
        "    \"linguistic\": [\"غبي\", \"جاهل\", \"متخلف\", \"إرهابي\", \"عنصري\", \"حقير\"],\n",
        "    \"religious\": {\n",
        "        \"مسيحي\": [\"كافر\", \"ضال\"],\n",
        "        \"شيعي\": [\"رافضي\", \"منحرف\"],\n",
        "        \"سني\": [\"ناصبي\", \"متشدد\"],\n",
        "        \"يهودي\": [\"ملعون\", \"مؤامرة\"]\n",
        "    },\n",
        "    \"cultural\": [\"بدائي\", \"غير متحضر\", \"أقل شأنا\", \"دوني\"]\n",
        "}\n",
        "\n",
        "# ------------------------------\n",
        "# Functions\n",
        "# ------------------------------\n",
        "def detect_ai_generated(text):\n",
        "    \"\"\"\n",
        "    Classify text as Human-written or AI-generated using zero-shot classification.\n",
        "    \"\"\"\n",
        "    labels = [\"Human-written\", \"AI-generated\"]\n",
        "    result = classifier(text, labels)\n",
        "    return result['labels'][0], result['scores'][0]\n",
        "\n",
        "def detect_bias(text):\n",
        "    \"\"\"\n",
        "    Detect bias in text:\n",
        "    - Linguistic\n",
        "    - Religious / Cultural\n",
        "    - Diversity of terms\n",
        "    \"\"\"\n",
        "    findings = []\n",
        "    bias_score = 0\n",
        "\n",
        "    # Check linguistic bias\n",
        "    for word in bias_keywords[\"linguistic\"]:\n",
        "        if word in text:\n",
        "            findings.append(f\"انحياز لغوي: يحتوي على كلمة '{word}'\")\n",
        "            bias_score += 15\n",
        "\n",
        "    # Check religious/cultural bias\n",
        "    for group, negatives in bias_keywords[\"religious\"].items():\n",
        "        for neg in negatives:\n",
        "            if neg in text:\n",
        "                findings.append(f\"انحياز ديني/ثقافي ضد '{group}': باستخدام كلمة '{neg}'\")\n",
        "                bias_score += 20\n",
        "\n",
        "    for word in bias_keywords[\"cultural\"]:\n",
        "        if word in text:\n",
        "            findings.append(f\"انحياز ثقافي: يحتوي على وصف '{word}'\")\n",
        "            bias_score += 10\n",
        "\n",
        "    # Sentiment check\n",
        "    sentiment = sentiment_analyzer(text[:512])[0]\n",
        "    if sentiment['label'] == \"NEGATIVE\":\n",
        "        findings.append(\"النص يحتوي على نبرة سلبية عامة\")\n",
        "        bias_score += 10\n",
        "\n",
        "    # Normalize bias score (0–100)\n",
        "    bias_score = min(bias_score, 100)\n",
        "\n",
        "    # Output result\n",
        "    if not findings:\n",
        "        findings.append(\"لا توجد مؤشرات واضحة على التحيز\")\n",
        "\n",
        "    return bias_score, findings\n",
        "\n",
        "# ------------------------------\n",
        "# Demo\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    sample_text = \"\"\"\n",
        "    التاريخ الإسلامي يوضح أن المذهب الشيعي كان دائما منحرفا عن الحق،\n",
        "    والسنة فقط هم أصحاب الحقيقة بينما الآخرون ضالون.\n",
        "    \"\"\"\n",
        "\n",
        "    # Detect AI vs Human\n",
        "    label, score = detect_ai_generated(sample_text)\n",
        "    print(\"🔍 تصنيف النص:\")\n",
        "    print(f\"- {label} (درجة الثقة: {score:.2f})\\n\")\n",
        "\n",
        "    # Detect Bias\n",
        "    bias_score, findings = detect_bias(sample_text)\n",
        "    print(\"⚖️ تحليل الانحياز:\")\n",
        "    print(f\"- درجة الانحياز: {bias_score}%\")\n",
        "    for f in findings:\n",
        "        print(f\"- {f}\")\n",
        "\n",
        "    # Example of summary\n",
        "    if bias_score > 0:\n",
        "        print(f\"\\n📌 النتيجة المبدئية: النص يحتوي على انحياز بنسبة {bias_score}% ضد المذهب الشيعي في عرضه للتاريخ الإسلامي.\")\n",
        "    else:\n",
        "        print(\"\\n📌 النتيجة المبدئية: النص لا يحتوي على انحياز واضح.\")\n"
      ],
      "metadata": {
        "id": "kZSUGQmMIZJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI-Generated Content and Bias Detection System with Harm Level Classification\n",
        "# Author: Shahd's Assistant\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. Load Models\n",
        "# -------------------------------------------------------------\n",
        "# Zero-shot classification for AI vs Human content\n",
        "ai_detector = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Sentiment analysis for detecting negative/biased tone\n",
        "sentiment_model = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Zero-shot classification for harm level\n",
        "harm_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. Define Bias Detection Function\n",
        "# -------------------------------------------------------------\n",
        "def detect_bias(text):\n",
        "    biases = {\n",
        "        \"Linguistic Bias\": [\"negative\", \"exclusionary\", \"offensive\", \"derogatory\"],\n",
        "        \"Religious/Cultural Bias\": [\"christianity\", \"islam\", \"judaism\", \"shiite\", \"sunni\", \"hindu\", \"buddhism\"],\n",
        "        \"Lack of Diversity\": [\"western perspective\", \"single source\", \"no diversity\"]\n",
        "    }\n",
        "\n",
        "    detected_biases = []\n",
        "    for bias_type, keywords in biases.items():\n",
        "        for word in keywords:\n",
        "            if re.search(r\"\\b\" + re.escape(word) + r\"\\b\", text.lower()):\n",
        "                detected_biases.append((bias_type, word))\n",
        "\n",
        "    return detected_biases\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. Harm Level Classification\n",
        "# -------------------------------------------------------------\n",
        "def classify_harm_level(text):\n",
        "    harm_labels = [\n",
        "        \"Misinformation\",\n",
        "        \"Cultural Exclusion\",\n",
        "        \"Religious Bias\",\n",
        "        \"Hate Speech\",\n",
        "        \"Sensitive Content\",\n",
        "        \"Neutral / Safe\"\n",
        "    ]\n",
        "    harm_result = harm_classifier(text, harm_labels)\n",
        "    return harm_result\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. Main Analysis Function\n",
        "# -------------------------------------------------------------\n",
        "def analyze_text(text):\n",
        "    # Step 1: Detect AI vs Human\n",
        "    ai_result = ai_detector(text, candidate_labels=[\"AI-generated\", \"Human-written\"])\n",
        "\n",
        "    # Step 2: Sentiment analysis\n",
        "    sentiment_result = sentiment_model(text)\n",
        "\n",
        "    # Step 3: Bias detection\n",
        "    detected_biases = detect_bias(text)\n",
        "\n",
        "    # Step 4: Harm level classification\n",
        "    harm_result = classify_harm_level(text)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Print results clearly in English\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== AI Content Detection ===\")\n",
        "    print(f\"Prediction: {ai_result['labels'][0]} with confidence {ai_result['scores'][0]:.2f}\")\n",
        "\n",
        "    print(\"\\n=== Sentiment Analysis ===\")\n",
        "    print(f\"Sentiment: {sentiment_result[0]['label']} (score: {sentiment_result[0]['score']:.2f})\")\n",
        "\n",
        "    print(\"\\n=== Bias Detection ===\")\n",
        "    if detected_biases:\n",
        "        for bias in detected_biases:\n",
        "            print(f\"- Potential {bias[0]} detected (keyword: {bias[1]})\")\n",
        "    else:\n",
        "        print(\"No explicit linguistic/religious/cultural bias detected.\")\n",
        "\n",
        "    print(\"\\n=== Harm Level Classification ===\")\n",
        "    for label, score in zip(harm_result['labels'], harm_result['scores']):\n",
        "        print(f\"- {label}: {score:.2f}\")\n",
        "\n",
        "    print(\"\\n=== Final Example Output ===\")\n",
        "    if detected_biases:\n",
        "        bias_example = f\"The text contains approximately {round(harm_result['scores'][0]*100, 1)}% indication of {harm_result['labels'][0]}.\"\n",
        "        print(bias_example)\n",
        "    else:\n",
        "        print(\"The text seems mostly neutral with minimal bias signals.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. Run Example\n",
        "# -------------------------------------------------------------\n",
        "sample_text = \"\"\"\n",
        "This article claims that one religious group is always wrong while ignoring other historical perspectives.\n",
        "It presents only a western perspective and excludes cultural diversity.\n",
        "\"\"\"\n",
        "\n",
        "analyze_text(sample_text)\n"
      ],
      "metadata": {
        "id": "F-Pcg4zGXL1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Load Models\n",
        "# ------------------------------------------------\n",
        "# Zero-shot classifier to detect AI vs Human written text\n",
        "ai_detector = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Sentiment analysis for linguistic bias\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Helper Functions\n",
        "# ------------------------------------------------\n",
        "\n",
        "def detect_ai_generated(text):\n",
        "    labels = [\"AI-generated\", \"Human-written\"]\n",
        "    result = ai_detector(text, labels)\n",
        "    return result\n",
        "\n",
        "def analyze_bias(text):\n",
        "    sentiment = sentiment_analyzer(text)[0]\n",
        "    sentiment_label = sentiment['label']\n",
        "    sentiment_score = sentiment['score']\n",
        "\n",
        "    # Simple keyword-based bias check (Arabic terms)\n",
        "    religious_bias = re.findall(r\"(شيعة|سنة|مسيحي|يهودي)\", text)\n",
        "    cultural_bias = re.findall(r\"(غرب|شرق|عربي|أوروبي)\", text)\n",
        "\n",
        "    bias_report = []\n",
        "    if religious_bias:\n",
        "        bias_report.append(f\"Religious/Cultural bias detected: {religious_bias}\")\n",
        "    if cultural_bias:\n",
        "        bias_report.append(f\"Cultural/Regional bias detected: {cultural_bias}\")\n",
        "\n",
        "    return {\n",
        "        \"sentiment\": sentiment_label,\n",
        "        \"sentiment_score\": round(sentiment_score, 3),\n",
        "        \"bias_details\": bias_report if bias_report else [\"No explicit bias detected.\"]\n",
        "    }\n",
        "\n",
        "def harm_level_classification(text):\n",
        "    labels = [\n",
        "        \"Misinformation\",\n",
        "        \"Cultural Exclusion\",\n",
        "        \"Sensitive Content\",\n",
        "        \"Neutral/Harmless\"\n",
        "    ]\n",
        "    result = ai_detector(text, labels)\n",
        "    return result\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Main Analysis\n",
        "# ------------------------------------------------\n",
        "def analyze_text(text):\n",
        "    ai_result = detect_ai_generated(text)\n",
        "    bias_result = analyze_bias(text)\n",
        "    harm_result = harm_level_classification(text)\n",
        "\n",
        "    print(\"=== Text Analysis Report ===\")\n",
        "    print(f\"Input Text: {text}\")\n",
        "\n",
        "    print(\"\\n[AI Detection]\")\n",
        "    print(f\"Prediction: {ai_result['labels'][0]} (Confidence: {round(ai_result['scores'][0], 3)})\")\n",
        "\n",
        "    print(\"\\n[Linguistic and Cultural Bias Detection]\")\n",
        "    print(f\"Sentiment: {bias_result['sentiment']} (Score: {bias_result['sentiment_score']})\")\n",
        "    for detail in bias_result['bias_details']:\n",
        "        print(f\"- {detail}\")\n",
        "\n",
        "    print(\"\\n[Harm Level Classification]\")\n",
        "    print(f\"Most likely category: {harm_result['labels'][0]} (Confidence: {round(harm_result['scores'][0], 3)})\")\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # Final Example Output (automatic summary)\n",
        "    # ------------------------------------------------\n",
        "    print(\"\\n=== Final Example Output ===\")\n",
        "    if bias_result['bias_details'] and \"No explicit bias detected.\" not in bias_result['bias_details']:\n",
        "        bias_example = f\"The text contains approximately {round(harm_result['scores'][0]*100, 1)}% indication of {harm_result['labels'][0]}.\"\n",
        "        print(bias_example)\n",
        "    else:\n",
        "        print(\"The text seems mostly neutral with minimal bias signals.\")\n",
        "    print(\"=============================\\n\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Test with Arabic Input (Results in English)\n",
        "# ------------------------------------------------\n",
        "test_text = \"التاريخ الإسلامي يركز فقط على إنجازات السنة دون ذكر مساهمات الشيعة.\"\n",
        "analyze_text(test_text)\n"
      ],
      "metadata": {
        "id": "hHcSNvf0ZnCQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}