{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# AI-Generated Content & Bias Detection System\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. Load Models\n",
        "# -------------------------------------------------------------\n",
        "# Zero-shot classification for AI vs Human\n",
        "ai_detector = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Sentiment analysis (English output)\n",
        "sentiment_model = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Zero-shot classification for harm and bias detection\n",
        "harm_bias_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. Define Bias Detection\n",
        "# -------------------------------------------------------------\n",
        "def detect_bias(text):\n",
        "    keywords = {\n",
        "        \"Religious Bias\": [\"شيعة\", \"سنة\", \"مسيحي\", \"يهودي\", \"إسلام\", \"يهودية\"],\n",
        "        \"Cultural Exclusion\": [\"غرب\", \"شرق\", \"عربي\", \"أوروبي\", \"single source\", \"no diversity\"]\n",
        "    }\n",
        "    detected = []\n",
        "    for label, words in keywords.items():\n",
        "        for w in words:\n",
        "            if re.search(r\"\\b\" + re.escape(w) + r\"\\b\", text):\n",
        "                detected.append((label, w))\n",
        "    return detected\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. Harm & Misinformation Classification\n",
        "# -------------------------------------------------------------\n",
        "def classify_harm(text):\n",
        "    labels = [\n",
        "        \"Misinformation\",\n",
        "        \"Cultural Exclusion\",\n",
        "        \"Religious Bias\",\n",
        "        \"Hate Speech\",\n",
        "        \"Sensitive Content\",\n",
        "        \"Neutral / Safe\"\n",
        "    ]\n",
        "    return harm_bias_classifier(text, candidate_labels=labels)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. Main Analysis\n",
        "# -------------------------------------------------------------\n",
        "def analyze_text(text):\n",
        "    # Step 1: AI vs Human\n",
        "    ai_result = ai_detector(text, candidate_labels=[\"AI-generated\", \"Human-written\"])\n",
        "\n",
        "    # Step 2: Sentiment\n",
        "    sentiment_result = sentiment_model(text)\n",
        "\n",
        "    # Step 3: Keyword Bias Detection\n",
        "    detected_biases = detect_bias(text)\n",
        "\n",
        "    # Step 4: Harm Level Classification\n",
        "    harm_result = classify_harm(text)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Print results in English\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== AI Content Detection ===\")\n",
        "    print(f\"Prediction: {ai_result['labels'][0]} (Confidence: {ai_result['scores'][0]:.2f})\")\n",
        "\n",
        "    print(\"\\n=== Sentiment Analysis ===\")\n",
        "    print(f\"Sentiment: {sentiment_result[0]['label']} (Score: {sentiment_result[0]['score']:.2f})\")\n",
        "\n",
        "    print(\"\\n=== Bias Detection (Religious, Cultural & Inclusivity) ===\")\n",
        "    if detected_biases:\n",
        "        for b in detected_biases:\n",
        "            print(f\"- {b[0]} detected (keyword: {b[1]})\")\n",
        "    else:\n",
        "        print(\"- No explicit religious/cultural bias or exclusion detected.\")\n",
        "\n",
        "    print(\"\\n=== Harm Level Classification ===\")\n",
        "    for label, score in zip(harm_result['labels'], harm_result['scores']):\n",
        "        print(f\"- {label}: {score:.2f}\")\n",
        "\n",
        "    print(\"\\n=== Final Assessment ===\")\n",
        "    if detected_biases:\n",
        "        bias_summary = f\"The text contains potential {detected_biases[0][0]} with approx {round(harm_result['scores'][0]*100,1)}% indication.\"\n",
        "        print(bias_summary)\n",
        "    else:\n",
        "        print(\"The text seems mostly neutral, culturally inclusive, and does not promote misinformation.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. Test Example\n",
        "# -------------------------------------------------------------\n",
        "sample_text = \"\"\"\n",
        "التاريخ الإسلامي يركز فقط على إنجازات السنة دون ذكر مساهمات الشيعة.\n",
        "يقدم منظورًا غربيًا فقط ويستبعد التنوع الثقافي.\n",
        "\"\"\"\n",
        "analyze_text(sample_text)"
      ],
      "metadata": {
        "id": "Ls47BcViPkCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}